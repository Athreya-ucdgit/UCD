---
title: "MRA - Assignment 2"
format: pdf
editor: visual
---

## Q3. Model Selection

```{r results='hide',message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(olsrr)
```

```{r}
crime_data = read_csv("Crimes.csv", show_col_types = FALSE)
crime_data
```

#### a) Use R to fit all possible models and compute AIC, BIC and $R^2_{\text{adj}}$ for each model. Report a table with your results.

```{r}
crime_model = lm(VR ~ ., data = crime_data)
tab = ols_step_all_possible(crime_model)
results = tab$result[,c("predictors","aic","sbc","adjr")]
knitr::kable(results)
```

#### b) Indicate the best model overall according to each of AIC, BIC and $R^2_{\text{adj}}$.

```{r}
print(c("AIC",results$predictors[which.min(results$aic)],
        results$aic[which.min(results$aic)]))
print(c("BIC",results$predictors[which.min(results$sbc)],
        results$sbc[which.min(results$sbc)]))
print(c("$R2_Adj",results$predictors[which.max(results$adjr)],
        results$adjr[which.max(results$adjr)]))
```

#### c) Implement a forward stepwise regression that uses BIC. You will start by fitting the null model (the model with no covariates) and computing its BIC. Then, consider all possible one covariate models and compute their BICs. Iterate until there is no improvement in your criteria.

```{r}
combinations = list(c("1"),
                    c("P","M","MR"),
                    c("M + P","MR + P","MR + M"),
                    c("MR + M + P"))
min = 100000000
min_combination = ""

for(i in 1:length(combinations)){
  flag = FALSE
  for(j in 1:length(combinations[[i]])){
    f = as.formula(paste("VR ~ ",paste(combinations[[i]][j])))
    crime_model = lm(f,data = crime_data)
    lm = sum(log(dnorm(crime_data$VR,fitted.values(crime_model),
                       sd = summary(crime_model)$sigma)))
    bic = -2 * lm + (i+1) * log(nrow(crime_data))
    if(bic < min){
      min = bic
      min_combination = combinations[[i]][j]
      flag = TRUE
    }
  }
  if(flag == FALSE){
    break
  }
}

print(c("Best Model" = min_combination, "Minimum BIC" = min))
```

#### d) Does the forward stepwise method find the best possible subset? Compare the solutions to item (a) and (c). Explain why solutions from stepwise regression might differ from the all possible regressions method in (a).

By looking at the BIC values, the best possible subset found using the all possible regressions was MR + M with a BIC value of 679.282922. The forward selection algorithm using the BIC found the same subset MR + M with a BIC value of 679.3748.

Forward and backward selection algorithms are greedy algorithms, meaning they explore a sequence of local improvements and stop when no further improvement is found. However, this greedy nature can prevent them from finding the global optimum.

For example, suppose there are four explanatory variables. The forward selection algorithm might identify a model with two variables as optimal because adding a third variable does not improve the model. As a result, it stops searching and does not evaluate the model with all four variables, which could potentially be the global optimum.

While these methods are efficient for large datasets with many explanatory variables—where evaluating all possible subsets is computationally infeasible—they may miss the global optimum. In this particular case, the forward selection algorithm did identify the global optimum subset but this cannot be guaranteed in every scenario.

#### e) Explain how you could implement a backward selection algorithm using the F test as a decision rule. You can assume that the confidence level is $\alpha$ = 0.05.

A backward selection algorithm using F-test as a decision rule can be implemented the same way we implement with BIC as the decision rule. Start with the complete model with all the explanatory variables and do the F-test with $\alpha$ = 0.05. This is our base.

**...write more**

## Q4) Multiple regression

```{r}
football = read_csv("football.csv", show_col_types = FALSE)
football
```

#### a) Fit a linear regression model relating the number of games won ($y$) to the team’s passing yardage ($x2$), the percentage of rushing plays ($x7$), and the opponents’ yards rushing ($x8$).

```{r}
football_model = lm(y ~ x2 + x7 + x8,data=football)
summary(football_model)
```

#### b) Compute the sum of squares.

```{r}
y_bar = mean(football$y)

sst = sum((football$y - y_bar)^2)
sse = sum((football$y - fitted.values(football_model))^2)
ssr = sum((fitted.values(football_model) - y_bar)^2)

cat("sse = ",sse," , ","ssr = ",ssr," , ","sst = ",sst)
```

The relationship between $SS_t$, $SS_e$, and $SS_r$:

$SS_t$ = $SS_e$ + $SS_r$.

```{r}
sse + ssr
```

```{r}
p = 3
n = nrow(football)
df_sse = n - p - 1
df_ssr = p
df_sst = df_sse + df_ssr

cat("DF of sse = ",df_sse," , ",
    "DF of ssr = ",df_ssr," , ",
    "DF of sst = ",df_sst)
```

```{r}
mst = sst / df_sst
mse = sse / df_sse
msr = ssr / df_ssr
cat("MSE = ",mse," , ","MSR = ",msr," , ","MST = ",mst)
```

#### c) Calculate the t statistics for testing the hypothesis $H0$ : $\beta_j$ = 0 versus $H1$ : $\beta_j$ $\neq$ 0 for $j$ = 1,2,3 where $\beta_1$, $\beta_2$, $\beta_3$ are the coefficients of $x_2$, $x_7$ and $x_8$.

```{r}
X = model.matrix(football_model)
beta = solve(t(X)%*%X)%*%t(X)%*%football$y
sigma_2 = sse / (n - p - 1)
sigma = sigma_2 * solve(t(X)%*%X)

t_b1 = beta[2,1] / sqrt(sigma[2,2])
t_b2 = beta[3,1] / sqrt(sigma[3,3])
t_b3 = beta[4,1] / sqrt(sigma[4,4])

print(c("T_" = t_b1, "T_" = t_b2, "T_" = t_b3))
```

#### d) Calculate $R^2$ and $R^2_\text{adj}$ using (b).

```{r}
r2 = 1 - (sse/sst)
r2_adj = 1 - (mse/mst)

print(c(r2 = r2,r2_adj = r2_adj))
```

#### e) Use item (b) to test the significance of the regression (F test). Outline the hypothesis of this test, compute the test statistic and conclude using the critical regions approach.

```{r}
f_stat = msr/mse
c(f_stat = f_stat,"95% CI F distribution" = qf(0.95,p, n - p - 1))
```

The F-statistic we have calculated is 29.4368. The 95% value of the F distribution for df = 3 and df = 24 is 3.008787. Since, the F is greater than the 0.95 quantile, we reject $H0$.

#### f) Show numerically that $R^2$ is equal to the square of the correlation coefficient between $Y_i$ and $\hat{Y_i}$.

```{r}
c("Square of correlation coefficent" = 
    cor(football$y, fitted.values(football_model))^2, "R2" = r2)
```

We see that the Square of the correlation coefficient is equal to the $R^2$ value.

#### g) Find a 95% CI on the mean number of games won by a team when $x_2$ = 2300, $x_7$= 56, $x_8$ = 2100.

```{r}
X_Star = matrix(c(1,2300,56,2100),ncol=1)

Y_Star_mean = t(X_Star)%*%matrix(beta,ncol=1)

Y_Star_mean_upper = Y_Star_mean + qt(0.95,n-p-1) * 
  sqrt(sigma_2 * t(X_Star)%*%solve(t(X)%*%(X))%*%X_Star)
Y_Star_mean_lower = Y_Star_mean - qt(0.95,n-p-1) * 
  sqrt(sigma_2 * t(X_Star)%*%solve(t(X)%*%(X))%*%X_Star)

c(Lower = Y_Star_mean_lower,Upper = Y_Star_mean_upper)
```

```{r}
Y_Star_mean_upper - Y_Star_mean_lower
```

The length of confidence interval is 1.29353.

#### h) Fit the model using $x_7$ and $x_8$ only and compute its error sums of squares.

```{r}
football_model_2 = lm(y ~ x7 + x8,data=football)
summary(football_model_2)
```

```{r}
y_bar_2 = mean(football$y)

sst_2 = sum((football$y - y_bar)^2)
sse_2 = sum((football$y - fitted.values(football_model_2))^2)
ssr_2 = sum((fitted.values(football_model_2) - y_bar)^2)

cat("sse = ",sse_2," , ","ssr = ",ssr_2," , ","sst = ",sst_2)
```

```{r}
p_2 = 2
n = nrow(football)
df_sse_2 = n - p_2 - 1
df_ssr_2 = p_2
df_sst_2 = df_sse_2 + df_ssr_2

cat("DF of sse = ",df_sse_2," , ",
    "DF of ssr = ",df_ssr_2," , ",
    "DF of sst = ",df_sst_2)
```

```{r}
mst_2 = sst_2 / df_sst_2
mse_2 = sse_2 / df_sse_2
msr_2 = ssr_2 / df_ssr_2
cat("MSE = ",mse_2," , ","MSR = ",msr_2," , ","MST = ",mst_2)
```

#### i) Perform an F test for hypothesis $H0$ : $\beta_2$ = $\beta_3$ = 0 versus $H1$ : at least one of $\beta_2$ or $\beta_3$ is different than zero.

```{r}
f_stat_2 = msr_2/mse_2
c(f_stat = f_stat_2,"95% CI F distribution" = qf(0.95,p_2,n -p_2 -1))
```

The F-statistic we have calculated is 29.4368. The 95% value of the F distribution for df = 2 and df = 25 is 3.008787. Since, the F is greater than the 0.95 quantile, we reject $H0$.

#### j) Recompute $R^2$ and $R^2_\text{adj}$ for the new model. How do these quantities compute to those in (d) ?

```{r}
r2_2 = 1 - (sse_2/sst_2)
r2_adj_2 = 1 - (mse_2/mst_2)

print(c(r2 = r2_2,r2_adj = r2_adj_2))
```

The $R^2$ and $R^2_\text{adj}$ value are lower than that of the previous model.

#### k) Recompute the 95% confidence interval on the mean number of games won by a team with the new model, using $x_7$ = 56, $x_8$ = 2100. Compare the length of the interval to (g).

```{r}
X = model.matrix(football_model_2)

beta = solve(t(X)%*%X)%*%t(X)%*%football$y
sigma_2 = sse / (n - p_2 - 1)
sigma = sigma_2 * solve(t(X)%*%X)

X_Star = matrix(c(1,56,2100),ncol=1)
Y_Star_mean = t(X_Star)%*%matrix(beta,ncol=1)

Y_Star_mean_upper = Y_Star_mean + qt(0.95,n - p_2 -1) * 
  sqrt(sigma_2 * t(X_Star)%*%solve(t(X)%*%(X))%*%X_Star)
Y_Star_mean_lower = Y_Star_mean - qt(0.95,n - p_2 - 1) * 
  sqrt(sigma_2 * t(X_Star)%*%solve(t(X)%*%(X))%*%X_Star)

c(lower = Y_Star_mean_lower,upper = Y_Star_mean_upper)
```

```{r}
Y_Star_mean_upper - Y_Star_mean_lower
```

The length of the confidence interval is 1.25138. It is slightly lower than that of the previous model.

#### l) Comment on how removing $x_2$ from the model changed the model adequacy and its predictions.

On removing $x_2$, the model started performing worse. We can very clearly see from the $SS_e$ value and $R^2$ values. The initial model had a lesser $SS_e$ and a higher $R^2$ value which signifies a better model.

## Q5) Types of variables

```{r}
bike_sharing = read.csv("bikesharing.csv")
```

#### a) Explain how to code the month of the year mnth using indicator variables.

```{r}
bike_sharing$mnth = as.factor(bike_sharing$mnth)
```

We use the as.factor() function to make the mnth variable as an indicator variable. January is reference category by default. We can print the levels for a factor with the levels() function. We can then pass this variable to the lm() function and it will automatically recognize it as an indicator variable.

```{r}
levels(bike_sharing$mnth)
```

#### b) Using R, fit a linear regression model to cnt using hum: the normalised measure of humidity, windspeed: the normalised wind speed on the day, temp: the normalised temperature, and mnth. Make sure to use mnth as a categorical variable using January as the reference category. Include the summary of the fitted model.

```{r}
bike_model = lm(cnt ~ hum + windspeed + temp + mnth, data = bike_sharing)
summary(bike_model)
```

#### c) Is there an indication that month of the year is an important variable?

```{r}
anova(bike_model)
```

The P value for the mnth variable is \< 2.2e-16. A low p-value (typically \< 0.05) indicates that the variable is statistically significant and contributes meaningfully to explaining the variance in the response variable.

#### d) What months of the year have a different average number of bike rentals in comparison to January, given hum, temp and windspeed? Use $\alpha$ = 0.05.

By looking at the summary chart of the model, we can see the following.

Months **with** significant difference compared to January, given Hum, Temp, and Windspeed:

**April, May, September, October, November, December.**

Months **without** significant difference compared to January, given Hum, Temp, and Windspeed

**February, March, June, July, August.**

#### e) Given hum = 0.4, temp = 0.3, windspeed = 0.65, what is the average number of rentals in each month? Report a table with your results.

```{r}
X_Star = matrix(nrow = 0,ncol = 15)
for(i in 1:12){
  x = c(1,0.4,0.3,0.65,0,0,0,0,0,0,0,0,0,0,0)
  if(i!=1){
    x[3+i] = 1
  }
  X_Star = rbind(X_Star,x)
}


result = X_Star%*%matrix(coef(bike_model))
dimnames(result) = list(c("Jan","Feb","Mar","Apr","May","Jun",
                          "Jul","Aug","Sep","Oct","Nov","Dec"),
                        c("Predicted value"))
knitr::kable(result)
```
